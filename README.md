# ğŸ“„ PDF Document Q&A (RAG System)

A Retrieval-Augmented Generation (RAG) application built with **LangChain**, **Chroma**, **MiniLM embeddings**, and **Google Gemini 2.5 Flash**.  
It allows you to ask natural language questions about research papers (PDFs), retrieves relevant chunks using local embeddings, and generates concise answers using Gemini.

---

## ğŸš€ Features

- ğŸ§  **Retrieval-Augmented Generation (RAG)** pipeline  
- ğŸ—‚ï¸ Automatic **PDF ingestion**, splitting, and embedding storage using **Chroma**  
- ğŸ” **MiniLM** (Sentence Transformers) for efficient, free local embeddings  
- ğŸ’¬ **Gemini 2.5 Flash** for high-quality answer generation  
- âš¡ **Streamlit** UI for simple interaction  
- ğŸ’¾ Local persistent **vector database** (`./chroma_db`)  
- ğŸ” Environment-based API configuration via `.env` file  
- ğŸ§© Modular and fully local (no external dependencies beyond Gemini API)

---

## ğŸ§° Tech Stack

| Component | Description |
|------------|--------------|
| **LangChain (modular)** | Framework for chaining retrieval + LLMs |
| **Sentence-Transformers (MiniLM)** | Local embedding model for text similarity |
| **ChromaDB** | Lightweight vector database for storing embeddings |
| **Google Gemini 2.5 Flash** | LLM used to generate final answers |
| **Streamlit** | Frontend web app for user queries |
| **Python-dotenv** | Loads environment variables securely |

---

## ğŸ“‚ Project Structure

Python Project/
â”œâ”€â”€ app.py # Streamlit app (retrieval + generation)
â”œâ”€â”€ ingest.py # Creates vector DB from PDFs
â”œâ”€â”€ utils.py (optional) # Helper functions (if used)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .env # Environment variables (Gemini API key, etc.)
â”œâ”€â”€ chroma_db/ # Persistent Chroma vector store
â”œâ”€â”€ pdf_qa_project/ # Folder containing paper1.pdf, paper2.pdf
â””â”€â”€ README.md # Project documentation

yaml
Copy code

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone or download the project

```bash
git clone <your_repo_url>
cd PDF-QA
2ï¸âƒ£ Create and activate a virtual environment
bash
Copy code
python -m venv venv
# Windows
venv\Scripts\activate
# macOS / Linux
source venv/bin/activate
3ï¸âƒ£ Install dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Add your environment variables
Create a .env file in the project root:

bash
Copy code
GOOGLE_API_KEY=your_google_api_key_here
PERSIST_DIRECTORY=./chroma_db
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
You can get your Gemini API key from Google AI Studio â†’ API Keys.

5ï¸âƒ£ Add your PDF files
Place your PDFs inside the pdf_qa_project/ folder and rename them:

bash
Copy code
pdf_qa_project/paper1.pdf
pdf_qa_project/paper2.pdf
ğŸ§© Step 1: Build the Vector Database
Run the ingestion script once to process and embed the PDFs.

bash
Copy code
python ingest.py
âœ… This script:

Reads both PDFs

Splits text into chunks

Embeds using MiniLM

Saves vectors to chroma_db/

ğŸ’¬ Step 2: Run the Streamlit Q&A App
bash
Copy code
streamlit run app.py
Once it starts, open the local URL (usually http://localhost:8501).
You can now type questions like:

"Summarize the key contributions of the first paper."
"What methods are compared in the second paper?"
"How do the authors evaluate performance improvements?"

ğŸ§  How It Works (Under the Hood)
PDF Loading & Chunking (ingest.py)

Loads PDFs using PyPDFLoader

Splits text into overlapping chunks (1000 characters, 200 overlap)

Embeddings

Converts chunks into dense vectors using MiniLM (sentence-transformers/all-MiniLM-L6-v2)

Vector Store (Chroma)

Stores and indexes embeddings for semantic search

Retrieval

Retrieves top-K chunks relevant to your question

Generation (Gemini)

Passes context + question to Gemini 2.5 Flash for answer synthesis

Streamlit Frontend

Displays the final answer, source chunks, and response time

ğŸ§¾ Example Output
vbnet
Copy code
Question: Summarize the key contributions.

âœ… Final Answer:
The key contributions include Arushi Somani conducting RL ablation experiments on CoT scoring with preference models...
ğŸ“– Context Used (Source Chunks):
Chunk 1: paper2.pdf, page 12
Chunk 2: paper2.pdf, page 12
...
ğŸ§° Troubleshooting
Issue	Fix
ModuleNotFoundError (langchain_*)	Run pip install -r requirements.txt again
GOOGLE_API_KEY missing	Check .env spelling and restart Streamlit
Failed to load Chroma vectorstore	Run python ingest.py first
Quota exceeded / API limit	Check Google AI Studio billing / quota page
Torch install fails on Windows	Try: pip install torch --index-url https://download.pytorch.org/whl/cpu

ğŸ§‘â€ğŸ’» Author
Manish Rawat
AI/ML Enthusiast â€” Building intelligent retrieval systems with LangChain and open-source embeddings.

ğŸ License
This project is open-source and free to use for educational and research purposes.